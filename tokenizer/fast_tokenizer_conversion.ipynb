{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert SentencePiece to FastTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load SPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_model = \"tokenizer.model\"\n",
    "sealion_spm = spm.SentencePieceProcessor(model_file=tokenizer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1173, 493, 384, 851, 613]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check\n",
    "sealion_spm.encode(\"There can be only one\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract vocab and merge from SPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256000/256000 [00:00<00:00, 1398410.88it/s]\n",
      "100%|██████████| 256000/256000 [2:09:21<00:00, 32.98it/s]  \n"
     ]
    }
   ],
   "source": [
    "%run sentencepiece_extractor --provider sentencepiece --model tokenizer/tokenizer.model --vocab-output-path tokenizer/vocab.json --merges-output-path tokenizer/merges.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load extracted vocab and merges into Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import SentencePieceBPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizer(vocabulary_size=256000, model=SentencePieceBPE, unk_token=<unk>, replacement=▁, add_prefix_space=True, dropout=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sealion_spBPE = SentencePieceBPETokenizer.from_file(merges_filename=\"tokenizer/merges.txt\", vocab_filename=\"tokenizer/vocab.json\")\n",
    "sealion_spBPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sealion_spBPE.save(\"tokenizer/sealion_spBPETokenizer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cast SentencePieceBPETokenizer to PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='', vocab_size=256000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sealion_tokenizer_fast = PreTrainedTokenizerFast(tokenizer_file=\"tokenizer/sealion_spBPETokenizer.json\")\n",
    "sealion_tokenizer_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1173, 493, 384, 851, 613], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check\n",
    "sealion_tokenizer_fast(\"There can be only one\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('SEABert_FastTokenizer_TMP/tokenizer_config.json',\n",
       " 'SEABert_FastTokenizer_TMP/special_tokens_map.json',\n",
       " 'SEABert_FastTokenizer_TMP/tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sealion_tokenizer_fast.save_pretrained(\"SEAlion_FastTokenizer_TMP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load PretrainedTokenizerFast via AutoTokenizer and assign Special Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='SEABert_FastTokenizer_TMP', vocab_size=256000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sealion_fasttokenizer_tmp = AutoTokenizer.from_pretrained(\"SEAlion_FastTokenizer_TMP\")\n",
    "sealion_fasttokenizer_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1173, 493, 384, 851, 613], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check\n",
    "sealion_fasttokenizer_tmp(\"There can be only one\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign PAD, BOS, EOS, MASK, UNK tokens\n",
    "sealion_fasttokenizer_tmp.pad_token_id = 0\n",
    "sealion_fasttokenizer_tmp.bos_token_id = 1\n",
    "sealion_fasttokenizer_tmp.eos_token_id = 2\n",
    "sealion_fasttokenizer_tmp.mask_token_id = 3\n",
    "sealion_fasttokenizer_tmp.unk_token_id = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|pad|>\n",
      "<|bos|>\n",
      "<|eos|>\n",
      "<|mask|>\n",
      "<|unk|>\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "print(sealion_fasttokenizer_tmp.pad_token)\n",
    "print(sealion_fasttokenizer_tmp.bos_token)\n",
    "print(sealion_fasttokenizer_tmp.eos_token)\n",
    "print(sealion_fasttokenizer_tmp.mask_token)\n",
    "print(sealion_fasttokenizer_tmp.unk_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign all other Special Tokens\n",
    "add_special_tokens = [\n",
    "    \"<|endofline|>\", \"\\n\", \"\\t\", \"\\r\", \"\\b\",]\n",
    "langs = [\n",
    "    \"<|en|>\", \"<|zh|>\", \"<|id|>\", \"<|ms|>\", \"<|tl|>\", \"<|my|>\",\n",
    "    \"<|th|>\", \"<|lo|>\", \"<|km|>\", \"<|ta|>\", \"<|vi|>\",\n",
    "    \"<|python|>\", \"<|javascript|>\", \"<|shell|>\", \"<|sql|>\"]\n",
    "for n in range(24, 1, -1):\n",
    "    add_special_tokens.append(\" \"*n)\n",
    "add_special_tokens.extend(langs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<|bos|>',\n",
       " 'eos_token': '<|eos|>',\n",
       " 'unk_token': '<|unk|>',\n",
       " 'pad_token': '<|pad|>',\n",
       " 'mask_token': '<|mask|>',\n",
       " 'additional_special_tokens': ['<|endofline|>',\n",
       "  '\\n',\n",
       "  '\\t',\n",
       "  '\\r',\n",
       "  '\\x08',\n",
       "  '                        ',\n",
       "  '                       ',\n",
       "  '                      ',\n",
       "  '                     ',\n",
       "  '                    ',\n",
       "  '                   ',\n",
       "  '                  ',\n",
       "  '                 ',\n",
       "  '                ',\n",
       "  '               ',\n",
       "  '              ',\n",
       "  '             ',\n",
       "  '            ',\n",
       "  '           ',\n",
       "  '          ',\n",
       "  '         ',\n",
       "  '        ',\n",
       "  '       ',\n",
       "  '      ',\n",
       "  '     ',\n",
       "  '    ',\n",
       "  '   ',\n",
       "  '  ',\n",
       "  '<|en|>',\n",
       "  '<|zh|>',\n",
       "  '<|id|>',\n",
       "  '<|ms|>',\n",
       "  '<|tl|>',\n",
       "  '<|my|>',\n",
       "  '<|th|>',\n",
       "  '<|lo|>',\n",
       "  '<|km|>',\n",
       "  '<|ta|>',\n",
       "  '<|vi|>',\n",
       "  '<|python|>',\n",
       "  '<|javascript|>',\n",
       "  '<|shell|>',\n",
       "  '<|sql|>']}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sealion_fasttokenizer_tmp.add_special_tokens({\"additional_special_tokens\": add_special_tokens})\n",
    "sealion_fasttokenizer_tmp.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('SEABert_FastTokenizer_Final/tokenizer_config.json',\n",
       " 'SEABert_FastTokenizer_Final/special_tokens_map.json',\n",
       " 'SEABert_FastTokenizer_Final/tokenizer.json')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save final version of tokenizer\n",
    "sealion_fasttokenizer_tmp.save_pretrained(\"SEAlion_FastTokenizer_Final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sealion_tokenizer = AutoTokenizer.from_pretrained(\"SEAlion_FastTokenizer_Final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check tokenizer type\n",
    "isinstance(sealion_tokenizer, PreTrainedTokenizerFast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256000"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check vocab size\n",
    "sealion_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [45, 371, 303, 4610, 4247, 337, 249923, 31], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check special tokens\n",
    "sealion_tokenizer(\"<|sql|> is a language written in <|en|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sealion_tokenizer.convert_tokens_to_ids(\"<|sql|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sealion_tokenizer.convert_tokens_to_ids(\"<|en|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [256001, 1173, 493, 384, 851, 613], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sealion_tokenizer(\"                        There can be only one\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256001"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sealion_tokenizer.convert_tokens_to_ids(\"                        \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [9975, 19723, 371, 6032], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sealion_tokenizer(\"Sea Lion is awesome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sea Lion is awesome'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sealion_tokenizer.decode([9975, 19723, 371, 6032])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
